{
    "collab_server" : "",
    "contents" : "---\ntitle: \"HW1\"\nauthor: \"Joaquin Rodriguez\"\ndate: \"9/26/2017\"\noutput:\n  pdf_document: default\n  html_document: default\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## 2.4 Table B.3 presents data on the gasoline mileage performance of 32 different automobiles.\n\n### a. Fit a simple linear regression model relating gasoline mileage y (miles per gallon) to engine displacement x1 (cubic inches).\n\n```{r}\nlibrary(MPV)\nlibrary(tidyverse)\nlibrary(MASS)\n\nauto <- table.b3\n\nnames(auto) <- c(\"Miles/gallon\", \"Displacement (cubic in)\", \"Horsepower (ft-lb)\", \"Torque (ft-lb)\", \"Compression ratio\", \"Rear axle ratio\", \"Carburetor (barrels)\", \"No. of transmission speeds\", \"Overall length (in)\", \"Width (in)\", \"Weight (lb)\", \"Type of transmission (1=automatic, 0=manual)\")\n\n\nfit1 <- lm(`Miles/gallon` ~ `Displacement (cubic in)`, data = auto)\n```\n\n### b. Construct the analysis-of-variance table and test for significance of regression.\n\n```{r}\nsum.fit1 <- summary(fit1)\nsum.fit1\nanova(fit1)\n```\n\n### c. What percent of the total variability in gasoline mileage is accounted for by the linear relationship with engine displacement?\n\nThe Adjusted R-squared for the linear model is `r round(sum.fit1$adj.r.squared, 3)`. Therefore, the engine displacement explains up to `paste(round(sum.fit1$adj.r.squared, 3), \"%\", sep = \"\")` of the variability of in the gasoline mileage data.\n\n### d. Find a 95% CI on the mean gasoline mileage if the engine displacement is 275 in.3\n\n```{r}\nnewdata2.4 <- data.frame(275)\nnames(newdata2.4) <- c(\"Displacement (cubic in)\")\npredict.lm(fit1, newdata2.4, interval = \"confidence\", level = 0.95)\n```\n\n### e. Suppose that we wish to predict the gasoline mileage obtained from a car with a 275-in.3 engine. Give a point estimate of mileage. Find a 95% prediction interval on the mileage.\n\n```{r}\npredict.lm(fit1, newdata2.4, interval = \"prediction\", level = 0.95)\n```\n\n### f. Compare the two intervals obtained in parts d and e. Explain the difference between them. Which one is wider, and why?\n\nThe prediction interval for the for the mean response provides the confidence interval for the response if we were to use a different sample from the one used to fit the regression. On the other hand, the confidence interval for the prediction estimates the CI for new observations, therefore considering the true error. As a consequence, the prediction interval will be wider as we have to consider the true error associated to the model. In fact, in the prediction formula we use an extra shock of root MSE to increase the width of the prediction.\n\n## 2.6 Table B.4 presents data for 27 houses sold in Erie, Pennsylvania.\n\n### a. Fit a simple linear regression model relating selling price of the house to the current taxes (x1).\n\n```{r}\nproperty <- table.b4\n\nnames(property) <- c(\"sale price of the house (in thousands of dollars)\",\"taxes (in thousands of dollars)\", \"number of baths\", \"lot size (in thousands of square feet)\", \"living space (in thousands of square feet)\", \"number of garage stalls\", \"number of rooms\", \"number of bedrooms\", \"age of the home (in years)\", \"number of fireplaces\")\n\nfit2 <- lm(`sale price of the house (in thousands of dollars)` ~ `taxes (in thousands of dollars)`, data = property)\nsum.fit2 <- summary(fit2)\n```\n\n### b. Test for significance of regression.\n\n```{r}\nsummary(fit2)\n```\n\n\n### c. What percent of the total variability in selling price is explained by this model?\n\n\nThe Adjusted R-squared for the linear model is `r round(sum.fit2$adj.r.squared, 3)`. Therefore, the taxes explains up to `r paste(round(sum.fit2$adj.r.squared, 3)*100, \"%\", sep = \"\")` of the variability of the sale price of the house.\n\n### d. Find a 95% CI on B1.\n```{r}\nconfint(fit2)\n```\n\n### e. Find a 95% CI on the mean selling price of a house for which the current taxes are $750.\n```{r}\nnewdata <- data.frame(750)\nnames(newdata) <-  c(\"taxes (in thousands of dollars)\")\npredict(fit2, newdata = newdata, interval = \"confidence\")\n```\n\n\n## 3.5 Consider the gasoline mileage data in Table B.3.\n\n### a. Fit a multiple linear regression model relatmg gasoline mileage y (miles per gallon) to engine displacement x1 and the number of carburetor barrels x6.\n\n```{r}\nfit3 <- lm(`Miles/gallon` ~ `Displacement (cubic in)` + `Carburetor (barrels)`, data = auto)\n\n```\n\n### b. Construct the analysis-of-variance table and test for significance of regression\n```{r}\nsummary(fit3)\nanova(fit3)\n```\n\n### c. Calculate R^2 and R^2 for this model. Compare this to the R^2 and the R^2Adj for the simple linear regression model relating mileage to engine displacement in Problem 2.4\n\n```{r}\n# full model\nfit3.summ <- summary(fit3)\nfit3.summ$r.squared\nfit3.summ$adj.r.squared\n\n# reduced model\nfit1.summ <- summary(fit1)\nfit1.summ$r.squared\nfit1.summ$adj.r.squared\n```\n\nThe R^2 and R^2adj for the full model are respectively: `r round(fit3.summ$r.squared,2)` and `r round(fit3.summ$adj.r.squared,2)`.\nWhereas, the R^2 and R^2adj for the reduced model are respectively: `r round(fit1.summ$r.squared,2)` and `r round(fit1.summ$adj.r.squared,2)`.  \nAs we can observe the R^2 and R^2adj for both models are nearly the same; the R^2 for the reduced model is slightly lower compared to the full one. The difference is around 1% and therefore the Carburetor variable does not significantly increase the portion of the variance explained by the model.\n\n### d. Find a 95% CI for B1.\n\n```{r}\nconfint(fit3)\n```\n\n### e. Compute the t statistics for testing H0: B1 = 0 and H0: B6 = 0. What conclusions can you draw?\n\n```{r}\nsummary(fit3)\n```\n\nThe Displacement predictor has a really low p-value, therefore we can reject the Null Hypothesis meaning that this beta is significantly different from zero.  \nThe Carburetor predictor has a high p-value, therefore we fail to reject the Null Hypothesis meaning that this beta is not significantly diffrent from zero. Therefore, the Carburetor predictor does not seem to add any significant information compared to the reduced model.  \n\n\n### f. Find a 95% CI on the mean gasoline mileage when x1 = 275 in.3 and x6 = 2 barrels.\n\n```{r}\nnewdata3.5 <- data.frame(275, 2)\nnames(newdata3.5) <- c(\"Displacement (cubic in)\", \"Carburetor (barrels)\")\npredict.lm(fit3, newdata3.5, interval = \"confidence\", level = 0.95)\n```\n\n### g. Find a 95% prediction interval for a new observation on gasoline mileage when x1 = 275 in.3 and x6 = 2 barrels.\n```{r}\npredict.lm(fit3, newdata3.5, interval = \"prediction\", level = 0.95)\n```\n\n## 3.6 In problem 2.4 you were asked to compute a 95% CI on mean gasoline prediction interval on mileage when the engine displacement x1 = 275 in.^3 Compare the lengths of these intervals to the lengths of the confidence and prediction intervals from Problem 3.5 above. Does this tell you anything about the benefits of adding x6 to the model?\n\n```{r}\n# reduced model\na <- predict.lm(fit1, newdata2.4, interval = \"prediction\", level = 0.95) %>% print()\na[1,3]- a[1,2] \n\n# full model\na <- predict.lm(fit3, newdata3.5, interval = \"prediction\", level = 0.95) %>% print()\na[1,3]- a[1,2] \n```\n\nAs we can observe interval length for the full model is slightly lower compared to the reduced model. Therefore, the full model is able to predict with a better confidence the mean gasoline consumption. This result is a consequence of the fact that the full model has a slightly higher adj R^2 compared to the reduced model. However, we can observe how the difference in interval length is quite small.  \n\n## 4.4 Consider the multiple regression model fit to the gasoline mileage data in Problem 3.5.\n\n```{r}\nplot(fit3)\n```\n\n### a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption?\n\nFrom the Normal Q-Q Plot we can observe how the assumption of normality appears to hold. There is a small departure from normaliy for some observations, however these departures do not seem to be significant enough to undermine the normality assumption.  \n\n### b. Construct and interpret a plot of the residuals versus the predicted response.\n\nFrom that standardized residual vs predicted response plot we can observe how the residuals are generally speaking equally spread along the ranges of the predictors. From the plot we can observe how there are three observations (#12, #15, #22) that have high residuals, thus indicating possible outliers or leverage points.\n\n### c. Construct and interpret the partial regression plots for this model.\n\n```{r}\nauto %>%\n  ggplot(aes(x = `Displacement (cubic in)`, y = `Miles/gallon`)) +\n  geom_point()\n```\n\nWe can observe how the relationship between Consumption vs Displacement approximates a simple linear trend. It is likely that the relationship would be better approximated by a second order polinomial.\n\n```{r}\nauto %>%\n  ggplot(aes(x = `Carburetor (barrels)`, y = `Miles/gallon`)) +\n  geom_point()\n```\n\nAs we can observe from the plot the relationship between Consumption and Carburetor do not seem to be correctly approximated by a linear regression. In fact, for each level of Carburetor we have significant variation. This is coherent with the fact that the Carburetor variable does not result to be significantly different from zero.\n\n### Compute the studentized residuals and the R-student residuals for this model. What information is conveyed by these scaled residuals?\n\n```{r}\n#studentized residuals\nresiduals(fit3) / fit3.summ$sigma * sqrt (1 - influence(fit3)$hat)\n\n#R-student residual\nrstudent(fit3)\n```\n\nThe studentized residuals provide a proper standardization of the errors as it considers also the hat values of the observations. In fact, the studentized residual follow closely a Z standard normal.  \nThe R-student provides a studentized residuals where the estemiate of sigma squared is estimated leaving out the current observation. Therefore, the R-student provides a externally studentized residuals.  \nFrom these residuals we can identify those observations that the model does not produce an adequate estimate. These measures are useful in order to identigy outliers and influential observations, but also deviations of the data from the underlying model assumed.  \n\n\n## 4.14 Problems 2.4 and 3.5 asked you to fit two different models to the gasoline mileage data in Table B.3. Calculate the PRESS statistic for these two models. Based on this statistic, which model is most likely to provide better predictions of new data?\n\n```{r}\n# PRESS 2.4\nsum( (resid(fit1) / (1- influence(fit1)$hat)) ^2 )\n\n# PRESS 3.5\nsum( (resid(fit3) / (1- influence(fit3)$hat)) ^2 )\n```\n\nThe PRESS conveys better predictions of new data for lower levels. Therefore, the model that is more likely predict better predictions of new data is the model we fit in point 3.5.",
    "created" : 1506480799561.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "253626736",
    "id" : "6914D1C8",
    "lastKnownWriteTime" : 1506989506,
    "last_content_update" : 1506989506426,
    "path" : "~/Library/Mobile Documents/com~apple~CloudDocs/LSU COURSES/EXST 7034/HW/HW1.Rmd",
    "project_path" : "HW1.Rmd",
    "properties" : {
        "chunk_output_type" : "console",
        "last_setup_crc32" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}