---
title: "HW3"
author: "Joaquin Rodriguez"
date: "11/28/2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(MKmisc)
library(survey)
library(tidyverse)
library(lmtest)
library(orcutt)
library(broom)
```

## 13.1 The table below presents the test-firing results for 25 surface-to-air antiair-craft missiles at targets of varying speed. The result of each test is either a hit (y = 1) or a miss (y = 0).

```{r}
air <- read.csv(file = "data/data-prob-13-1.csv")
```

### a. Fit a logistic regression model to the response variable y. Use a simple linear regression model as the structure for the linear predictor.

```{r}
logit1 <- glm(formula = y~x, data = air, family = binomial)
summary(logit1)
```

### b. Does the model deviance indicate that the logistic regression model from part a is adequate?

```{r}
HLgof.test(fit = fitted(logit1), obs = air$y)
```

As we can observe from the Hosmer-Lemeshow Test the p-value is above 0.05, therefore the model provides a good fit of the data.

### c. Provide an interpretation of the parameter B1 in this model.

For a unit increase in speed the log-ods of hitting the target decreases by `r logit1$coefficients[2]`. If we exponentiate the coefficient we can interpret it as the change in odds of hitting the target for a unit increase in speed is `r exp(logit1$coefficients[2])`.

### d. Expand the linear predictor to include a quadratic term in target speed. Is there any evidence that this quadratic term is required in the model?

```{r}
logit2 <- glm(formula = y~x+I(x^2), data = air, family = binomial)

anova(logit1, logit2, test ="Chisq")
```

As the p-value for the test is greater than 0.05 we do not have evidence agains the null hypothesis. Therefore, the reduced model is preferred, as the quadratic term does not significantly contributes to improve the model.  

## 13.5 A study was performed to investigate new automobile purchases. A sample of 20 families was selected. Each family was surveyed to determine the age of their oldest vehicle and their total family income. A follow-up survey was conducted 6 months later to determine if they had actually purchased a new vehicle during that time period (y = 1 indicates yes and y = 0 indicates no). The data from this study are shown in the following table.

```{r}
auto <- read.csv(file = "data/data-prob-13-5.csv")
```

### a. Fit a logistic regression model to the data.
```{r}
logit3 <- glm(formula = y~x1+x2, data = auto, family = binomial)
summary(logit3)
```

### b. Does the model deviance indicate that the logistic regression model from part a is adequate?

```{r}
HLgof.test(fit = fitted(logit3), obs = auto$y)
```

As we can observe from the Hosmer-Lemeshow Test the p-value is above 0.05, therefore the model provides a good fit of the data.

### c. Interpret the model coefficients B1 and B2.  
For a unit increase in income the log-ods of buying a car increases by `r logit3$coefficients[2]`. If we exponentiate the coefficient we can interpret it as the change in odds of buying a car for a unit increase in income is `r exp(logit3$coefficients[2])`.
For a unit increase in age of the car the log-ods of buying a car increases by `r logit3$coefficients[3]`. If we exponentiate the coefficient we can interpret it as the change in odds of buying a car for a unit increase in car age is `r exp(logit3$coefficients[3])`.

### d. What is the estimated probability that a family with an income of $45,000 and a car that is 5 years old will purchase a new vehicle in the next 6 months?

```{r}
df <- data.frame(x1 = 45000, x2 = 5)
exp(predict(logit3 , newdata = df)) / (1+exp(predict(logit3 , newdata = df)))
```

The estimated probability that a family with $45,000 income and 5 years old car will purchase a new vehicle is `r exp(predict(logit3 , newdata = df)) / (1+exp(predict(logit3 , newdata = df)))`.

### e. Expand the linear predictor to include an interaction term. Is there any evidence that this term is required in the model?
```{r}
logit4 <- glm(formula = y~x1*x2, data = auto, family = binomial)

anova(logit3, logit4, test ="Chisq")
```

The p-value of the test is <0.05, therefore we can reject the null hypothesis. This means that we favour the full model (with interaction term) over the reduced model.

### f. For the model in part a, find statistics for each individual model parameter.
```{r}
summary(logit3)
```


### d. Find approximate 95% confidence intervals on the model parameters for the logistic regression model from part a.
```{r}
confint(logit3, level = 0.05)
```

## 13.6 A chemical manufacturer has maintained records on the number of failures of a particular type of valve used in its processing unit and the length of time (months) since the valve was installed. The data are shown below.

```{r}
failures <- read.csv(file = "data/data-prob-13-6.csv")
```

### a. Fit a Poisson regression model to the data.
```{r}
poisson1 <- glm(num.fail ~ months, family = poisson, data = failures)
summary(poisson1)
```

### b. Does the model deviance indicate that the Poisson regression model from part a is adequate?
```{r}
pchisq(poisson1$deviance, df=poisson1$df.residual, lower.tail=FALSE)
```

He fail to reject the null hypothesis, therefore our model fits the data well.

### c. Construct a graph of the fitted model versus months. Also plot the observed number of failures on this graph.
```{r}
poisson1 %>%
  augment() %>% 
  mutate(.fitted = exp(.fitted)) %>% 
  select(num.fail, months, .fitted) %>%
  gather(key = "Measure", value = "Value", -months) %>% 
  ggplot(aes(months, Value))+
  geom_col()+
  facet_grid( ~ Measure)
```

### d. Expand the linear predictor to include a quadratic term. Is there any evidence that this term is required in the model?
```{r}
poisson2 <- glm(num.fail ~ months + I(months^2), family = poisson, data = failures)
anova(poisson1, poisson2, test ="Chisq")
```

The p-value of the test is <0.05, therefore we can reject the null hypothesis. This means that we favour the full model (with quadratic term) over the reduced model.

### e. For the model in part a, find Wald statistics for each individual model parameter.
```{r}
regTermTest(poisson1, "months")
```

He reject the null hypothesis, therefore the variable is significant in order to preserve the fit of the model.

### f. Find approximate 95% confidence intervals on the model parameters for the Poisson regression model from part a.
```{r}
confint(poisson1, level = 0.95)
```


## 13.7 Myers [1990] presents data on the number of fractures (y) that occur in the upper seams of coal mines in the Appalachian region of western Virginia. Four regressors were reported: x1 = inner burden thickness (feet), the shortest distance between seam floor and the lower seam; x2 = percent extraction of the lower previously mined seam; x3 = lower seam height (feet); and x4 = time (years) that the mine has been in operation. The data are shown below.

```{r}
fractures <- read.csv(file = "data/data-prob-13-7.csv")
```

### a. Fit a Poisson regression model to these data using the log link.
```{r}
poisson3 <- glm(y ~ x1 + x2 + x3 + x4, family = poisson, data = fractures)
summary(poisson3)
```

### b. Does the model deviance indicate that the model from part a is satisfactory?
```{r}
pchisq(poisson3$deviance, df=poisson3$df.residual, lower.tail=FALSE)
```

We fail to reject the null hypothesis, therefore our model fit the data well.

### c. Perform a type 3 partial deviance analysis of the model parameters. Does this indicate that any regressors could be removed from the model?
```{r}
poisson4 <- glm(y ~ x1 + x2 + x3, family = poisson, data = fractures)
poisson5 <- glm(y ~ x1 + x2 + x4, family = poisson, data = fractures)
poisson6 <- glm(y ~ x1 + x3 + x4, family = poisson, data = fractures)
poisson7 <- glm(y ~ x2 + x3 + x4, family = poisson, data = fractures)

anova(poisson3, poisson4, test = "Chisq")
anova(poisson3, poisson5, test = "Chisq")
anova(poisson3, poisson6, test = "Chisq")
anova(poisson3, poisson7, test = "Chisq")
```

The p-value of the test is <0.05, therefore we can reject the null hypothesis. This means that we favour the full model (with quadratic term) over the reduced model.


The p-value for **x4** is > 0.05, therefore we fail to reject the null hypothesis, meaning that the variable can be removed from the model without significantly loosing any goodness of fit.  

The p-value for **x3** is > 0.05, therefore we fail to reject the null hypothesis, meaning that the variable can be removed from the model without significantly loosing any goodness of fit.  

The p-value for **x2** is < 0.05, therefore we reject the null hypothesis, meaning that the variable cannot be removed from the model without significantly loosing goodness of fit.  

The p-value for **x1** is > 0.05, therefore we fail to reject the null hypothesis, meaning that the variable can be removed from the model without significantly loosing any goodness of fit.  

### d. Compute Wald statistics for testing the contribution of each regressor to the model. Interpret the results of these test statistics.

```{r}
regTermTest(poisson3, "x1")
regTermTest(poisson3, "x2")
regTermTest(poisson3, "x3")
regTermTest(poisson3, "x4")
```

We reject the null hypothesis for variables x1 and x2, therefore these two variables are significant in order to preserve the fit of the model.  
We fail to reject the null hypothesis for variables x3, and x4. Therefore these two variables do not contribute significantly to the goodness of the model.

### e. Find approximate 95% Wald confidence intervals on the model parameters.
```{r}
confint(poisson3, level = 0.95)
```

## 14.3 The data in the table below give the percentage share of market of a particular brand of canned peaches (yt) for the past 15 months and the relative selling price (xt).

```{r}
peaches <- read.csv(file = "data/14.3.csv")
```

### a.  Fit a simple linear regression model to these data. Plot the residuals versus time. Is there any indication of autocorrelation?
```{r}
lm1 <- lm(y~x, data = peaches)
```

```{r}
data.frame(peaches$t, lm1$residuals) %>% 
  ggplot(aes(peaches.t, lm1.residuals))+
  geom_point()+
  geom_line()
```

From the above plot we can observe how there's appers to be an autocorrelation of the error terms. In fact, we can observe what appers to be a seasonal effect in the data as the are moths in which the negative or positive trend of the previous months seems to influence the current one. 

### b. Use the Durbin–Watson test to determine if there is positive autocorrelation in the errors. What are your conclusions?
```{r}
dwtest(y~x, data = peaches)
```

The Durbin-Watson confirms that there is an autocorrelation of the error terms in the fitted model.

### c. Use one iteration of the Cochrane–Orcutt procedure to estimate the regression coefficients. Find the standard errors of these regression coefficients.

```{r}
summary(cochrane.orcutt(lm1))
```

### d. Is there positive autocorrelation remaining after the first iteration? Would you conclude that the iterative parameter estimation technique has been successful?
From the above output we can observe how the Durbin-Watson test is has a p-value < 0.05. Therefore, it appers that the autocorrelation is still present in the errors. This might indicate the presese of a second order autocorrelation.